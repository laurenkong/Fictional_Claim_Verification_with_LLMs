# Fictional Claim Verification with LLMs
Stanford CS224N Natural Language Processing with Deep Learning Final Project

Read our research paper here: 
https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1244/final-projects/JenniferJingXuLaurenYumiKong.pdf

## Abstract
While non-fiction claim verification has become a popular language modeling task, 
claim verification in fictional narratives represents a novel challenge, with few
datasets and limited research. This paper pioneers the application of large language
models (LLMs) for claim verification tasks for fictional narratives, assessing their
understanding of narrative coherence by identifying continuity and unresolved
errors. Addressing the lack of suitable datasets inhibiting advancement in this re-
search area, we present a novel, synthetically-generated dataset encompassing both
error types. Our results reveal that LLMs, specifically a cumulatively fine-tuned
version of Mistral 7B v0.2 - first on creative writing generation and subsequently
on fictional claim verification tasks - significantly outperform existing methods.
Our findings underscore the categorical well-suitedness and potential of LLMs
in identifying narrative consistency errors, providing groundwork toward a more
complex goal of claim verification in fictional narratives.
