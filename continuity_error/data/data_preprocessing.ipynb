{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDX3GA5NAReo",
        "outputId": "8b417c5f-9607-407d-c3e7-0214da182b58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.12.0-py3-none-any.whl (226 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/226.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m163.8/226.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.2)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.12.0\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import random\n",
        "!pip install openai\n",
        "from openai import OpenAI\n",
        "\n",
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=) #insert openai api key here"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "reddit_dataset = '/content/drive/My Drive/continuity_error_updated_stories.csv'\n",
        "df = pd.read_csv(reddit_dataset)"
      ],
      "metadata": {
        "id": "KDamrMk6Jh8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(reddit_dataset, 'r') as file:\n",
        "    stories = file.readlines()"
      ],
      "metadata": {
        "id": "LLCcotPkJm_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_stories = random.sample(stories, 1000)"
      ],
      "metadata": {
        "id": "4qe0nF4KJ1oR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(selected_stories, columns=['story'])\n",
        "df['error'] = None\n",
        "df['position'] = None\n",
        "df['response'] = None\n",
        "df"
      ],
      "metadata": {
        "id": "dsBiupYjOWPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in df.iterrows():\n",
        "    if pd.isnull(row['error']):\n",
        "        prompt = \"Pretend you were the author of the subsequent story. You want to insert one major continuity error, a lapse in the self-consistency of the narrative, somewhere in the story you've written (one sentence long), that a reader would only notice if they are paying close attention. Return two things: the sentence you'd add and the number of where you would insert it (count the number of '<nl>' that came before it in the story).\"\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt + row['story']}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        completion_text = response.choices[0].message.content\n",
        "        if '#' in completion_text:\n",
        "          error, position = completion_text.rsplit('#', 1)\n",
        "          position = position.strip()\n",
        "        else:\n",
        "          continue\n",
        "\n",
        "        df.at[index, 'error'] = completion_text\n",
        "        df.at[index, 'position'] = position\n",
        "        df.at[index, 'response'] = completion_text\n",
        "\n",
        "        print (completion_text)\n",
        "\n",
        "        df.to_csv('updated_stories.csv', index=False)"
      ],
      "metadata": {
        "id": "pkKWB9wRM73m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to strip quotes, strip white space, and add \"<nl>\" if not present\n",
        "def clean_error_text(text):\n",
        "    text = text.strip('\"')\n",
        "    text = text.strip()\n",
        "    if not text.endswith('<nl>'):\n",
        "        text += '<nl>'\n",
        "    return text\n",
        "\n",
        "df['error'] = df['error'].apply(clean_error_text)\n",
        "df.to_csv('updated_stories.csv', index=False)\n",
        "df['error']"
      ],
      "metadata": {
        "id": "Ym8tlfjSiica"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resuming, Data Cleaning/Processing"
      ],
      "metadata": {
        "id": "76xBPIdeFZ4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive', force_remount=True)\n",
        "reddit_dataset = '/content/drive/My Drive/updated_stories(7).csv'\n",
        "df = pd.read_csv(reddit_dataset)"
      ],
      "metadata": {
        "id": "gfwgttDGmI8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def error_insert(row):\n",
        "    position = int(row['position'])\n",
        "    story_parts = row['story'].split('<nl>')\n",
        "    insert_position = min(position, len(story_parts))\n",
        "    story_parts.insert(insert_position, row['error'])\n",
        "    return '<nl>'.join(story_parts)\n",
        "\n",
        "df['modified'] = df.apply(error_insert, axis=1)"
      ],
      "metadata": {
        "id": "tB92JuiNngcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('continuity_error_updated_stories.csv', index=False)"
      ],
      "metadata": {
        "id": "6GP-vmpSptd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split into Train/Dev/Test (80/10/10)"
      ],
      "metadata": {
        "id": "kjZ1wdX6Fhto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "reddit_dataset = '/content/drive/My Drive/continuity_error_updated_stories.csv'\n",
        "df = pd.read_csv(reddit_dataset)\n",
        "\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Split the dataset\n",
        "train_size = 0.8\n",
        "validation_size = 0.1\n",
        "test_size = 0.1\n",
        "\n",
        "train_df, temp_df = train_test_split(df, train_size=train_size, random_state=42)\n",
        "validation_df, test_df = train_test_split(temp_df, test_size=test_size / (test_size + validation_size), random_state=42)\n",
        "\n",
        "train_df.to_csv('train_dataset.csv', index=False)\n",
        "validation_df.to_csv('validation_dataset.csv', index=False)\n",
        "test_df.to_csv('test_dataset.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6lnjfIQFhQK",
        "outputId": "be5757d3-a3e2-4029-8ee7-fa068302030c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ]
}