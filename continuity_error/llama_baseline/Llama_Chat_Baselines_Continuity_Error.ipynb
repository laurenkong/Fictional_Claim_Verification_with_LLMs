{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate bitsandbytes\n",
        "!pip install torch -U\n",
        "!pip install transformers -U"
      ],
      "metadata": {
        "id": "ZgGyWYanlvr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from sklearn.metrics import f1_score, mean_squared_error\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "xJS5-ZmXHxEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "id": "v14Iz1bAJYHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model and tokenizer\n",
        "# model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "model_name = \"meta-llama/Llama-2-13b-chat-hf\"\n",
        "access_token = \"\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    load_in_4bit=True,\n",
        "    low_cpu_mem_usage=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    use_auth_token=access_token,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, use_auth_token=access_token)"
      ],
      "metadata": {
        "id": "W9Zu7FomHiT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the test dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "test_csv_path = '/content/drive/My Drive/test_dataset.csv'\n",
        "test_df = pd.read_csv(test_csv_path)"
      ],
      "metadata": {
        "id": "KB6FPuNhHy9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = []\n",
        "y_pred = []\n",
        "responses = []\n",
        "\n",
        "for index, row in tqdm(test_df.iterrows(), total=test_df.shape[0]):\n",
        "    story = row['modified']\n",
        "    position = row['position']\n",
        "    prompt = f\"There is one major continuity error, a lapse in the self-consistency of the narrative, in the story provided. Count the number of '<nl>' in the story that occurs before the continuity error. Return that number only. Do not return any other information in your response. Story: {story}\"\n",
        "\n",
        "    # Generate model output\n",
        "    model_inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
        "    output = model.generate(**model_inputs)\n",
        "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    print(response)\n",
        "    responses.append(response)\n",
        "\n",
        "    # Extract predicted position from the response\n",
        "    try:\n",
        "        predicted_position = int(''.join(filter(str.isdigit, response)))  # Extracts digits and converts to integer\n",
        "        y_pred.append(predicted_position)\n",
        "    except ValueError:\n",
        "        y_pred.append(-1)  # Handle non-numeric responses\n",
        "\n",
        "    y_true.append(position)"
      ],
      "metadata": {
        "id": "7SWiG94TNVwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_response = pd.DataFrame(responses, columns=['response'])\n",
        "df_preds = pd.DataFrame(y_pred, columns=['preds'])\n",
        "df_true = pd.DataFrame(y_true, columns=['true'])\n",
        "\n",
        "df_response.to_csv(\"test_responses_13b.csv\", index=False)\n",
        "df_preds.to_csv(\"test_preds_13b.csv\", index=False)\n",
        "df_true.to_csv(\"test_true_13b.csv\", index=False)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"test_preds_13b.csv\")\n",
        "files.download(\"test_true_13b.csv\")\n",
        "files.download(\"test_responses_13b.csv\")"
      ],
      "metadata": {
        "id": "s8m5ZQMkowEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract quantitiative data from LLM output"
      ],
      "metadata": {
        "id": "7BQQZsGOGgDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas openpyxl"
      ],
      "metadata": {
        "id": "TYGydN10HSl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load the test dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "df = pd.read_excel(\"/content/drive/MyDrive/test_continuity_error.xlsx\", engine='openpyxl')"
      ],
      "metadata": {
        "id": "XRBAJRgaG2-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def extract_position(response):\n",
        "    try:\n",
        "        sentences = re.split(r'\\.\\s+', response.strip())\n",
        "        last_sentence = sentences[-1]\n",
        "        position = re.findall(r'\\d+', last_sentence)\n",
        "        if position:\n",
        "            return int(position[-1])\n",
        "        else:\n",
        "            return 0  # In case no number is found\n",
        "    except Exception:\n",
        "        return 0\n",
        "\n",
        "df_response['llama_13b'] = df_response['response'].apply(extract_position)\n",
        "df_response['true'] = df_true[\"true\"]\n",
        "\n",
        "df_response.to_csv('cleaned_responses_13b.csv', index=False)\n",
        "files.download('cleaned_responses_13b.csv')"
      ],
      "metadata": {
        "id": "GIT7iNMeGodR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run Evals"
      ],
      "metadata": {
        "id": "RCeDoy8WMS6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/test_llama_continuity_error.csv\")"
      ],
      "metadata": {
        "id": "6xkbpc2JMSoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Convert true and predicted positions to arrays for scoring\n",
        "y_true = df_true[\"true\"]\n",
        "y_pred = df_response[\"llama_13b\"]\n",
        "\n",
        "# Classification Metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='weighted', zero_division=1)\n",
        "recall = recall_score(y_true, y_pred, average='weighted', zero_division=1)\n",
        "f1 = f1_score(y_true, y_pred, average='weighted', zero_division=1)\n",
        "\n",
        "# Regression Metrics\n",
        "mse = mean_squared_error(y_true, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "r_squared = r2_score(y_true, y_pred)\n",
        "\n",
        "# Print all metrics\n",
        "print(f'Classification Metrics:\\nAccuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\\nF1 Score: {f1}')\n",
        "print(f'Regression Metrics:\\nMSE: {mse}\\nRMSE: {rmse}\\nMAE: {mae}\\nR-squared: {r_squared}')"
      ],
      "metadata": {
        "id": "V0jvEVzoMskR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "i69UDQnA5Cbq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}